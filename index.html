<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ü§ñüí¨ + üìö + ‚òïÔ∏è | ChatGPT@ascii</title>
  <link rel="stylesheet" href="simple.min.css">
  <script defer>
    const generatePapersListHTML = (papers) => {
      return `
    <ul>
      ${papers.map(paper => `<li><a href="${paper.link}">${paper.title}</a></li>`).join('')}
    </ul>
  `;
    };

    fetch('meetings.json')
      .then(response => response.json())
      .then(data => {
        const { nextMeeting, pastMeetings } = data;

        const { date, time, room, papers } = nextMeeting;
        document.getElementById("next-meeting-date").textContent = data.nextMeeting.date;
        document.getElementById("next-meeting-time").textContent = data.nextMeeting.time;
        document.getElementById("next-meeting-room").textContent = data.nextMeeting.room;

        const nextMeetingsPaperList = document.getElementById("next-meeting-papers");
        nextMeetingsPaperList.innerHTML = generatePapersListHTML(papers);

        const pastMeetingsList = document.getElementById("past-meetings-list");
        pastMeetings.forEach(meeting => {
          pastMeetingsList.innerHTML += `
            <li>${meeting.date}, ${meeting.time}, ${meeting.room} (<a href="${meeting.slides}" target="_blank"
          rel="noopener noreferrer">Slides</a>): ${generatePapersListHTML(meeting.papers)}</li>
          `;
        });
      });
  </script>
</head>

<body>
  <header>
    <h2>ChatGPT Reading Group @ ascii</h2>
    <p>ü§ñüí¨ + üìö + ‚òïÔ∏è</p>
  </header>

  <main>
    <section>
      <h4>N√§chstes Treffen</h4>
      <p>
        Du kannst jetzt √ºber den Zeitpunkt des n√§chsten Treffens <a href="https://dud-poll.inf.tu-dresden.de/KDY6N67jjg/">hier abstimmen</a>.
        Um den Einstieg zu erleichtern, wollen wir noch einmal <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> lesen.
      </p>
      <p id="next-meeting">
        Das n√§chste Treffen findet am <span id="next-meeting-date">...</span> um <span id="next-meeting-time">...</span>
        im Raum <span id="next-meeting-room">...</span> statt.
        Bitte lest bis dahin:
      <ul id="next-meeting-papers">
        <!-- Filled by JavaScript -->
      </ul>
      </p>
      <noscript>
        <p>‚ö†Ô∏èDie Informationen zum n√§chsten Treffen konnten nicht geladen werden, da JavaScript deaktiviert ist.</p>
      </noscript>
    </section>

    <p>Hallo zusammen!</p>
    <p>
      Es gab mal die Idee, gemeinsam die grundlegenden Paper, auf denen ChatGPT aufbaut, zu lesen und dr√ºber
      zu reden. Das Ganze ist jetzt keine ascii-interne Veranstaltung, aber ich hatte im ascii schonmal mit Leuten
      dar√ºber gesprochen und das Interesse war scheinbar da. Um das mal zu besprechen, w√ºrde ich vorschlagen,
      dass wir uns mal im ascii treffen. Danach k√∂nnen wir auch schauen, ob/wie wir es weiter bewerben wollen.
      Das ascii als Caf√© scheint mir aber erstmal ein geeigneter Ort f√ºr so eine Veranstaltung zu sein.
    </p>
    <p>
      F√ºr ein erstes Treffen habe ich ein dudle zur Terminfindung rum geschickt. Wer das nicht bekommen hat, kann
      gerne eine E-Mail an <code>reading-group(at)deep.cooking</code> schicken. Das erste Treffen kann auch erstmal rein
      organisatorisch sein. Dann k√∂nnen wir mal sehen, wie viele wir eigentlich sind und ob das Format (siehe unten)
      so passt. Nur der thematische Fokus ist fest.
    </p>

    <h4>"Reading Group"</h4>
    <p>
      Die Idee w√§re, dass wir uns regelm√§√üig (zwei w√∂chentlich?) zusammen setzen. Pro Termin gibt es ein Paper o.√§.,
      das <b>alle vorher lesen</b> und im Idealfall auch direkt ein paar offene Punkte, Unklarheiten oder sonstige
      Fragen
      aufschreiben. Eine Person stellt das Paper dann anhand von ein paar Slides vor (da reichen auch die
      Abbildungen und Tabellen in eine Pr√§sentation.). Wichtig ist eben, dass wir f√ºr jedes Paper eine/n
      Verantwortliche/n
      haben, um Fragen zu stellen und ein bisschen den Termin zu strukturieren. Das ist aber auch einfacher als es
      klingt :) Das erste Papier kann ich gerne selbst √ºbernehmen.
    </p>

    <h5>
      Fokus
    </h5>
    <p>
      Als Fokus f√ºr die Reading Group h√§tte ich, wie oben geschrieben, gerne alles was irgendwie mit ChatGPT und
      den aktuellen Sprachmodellen, die f√ºr Chats und Co. benutzt werden, zu tun hat. Es gibt ja eine Reihe von
      Weiterentwicklungen und Verbesserungen. Die w√ºrde ich mir gerne mal anschauen.
    </p>

    <p>
      Das Ganze soll also <b>keine Lehrveranstaltung</b> sein, auch <b>kein Programmierkurs</b> oder √§hnliches. Einfach
      nur Paper
      lesen und dann bei einem Kaffee, Tee oder einem Kaltgetr√§nk dar√ºber reden.
    </p>

    <h5>
      Paper
    </h5>
    <p>
      Als Paper w√ºrde ich folgende vorschlagen (nicht zwingend in der Reihenfolge):
    </p>

    <ul>
      <li>Transformers: Attention is All You Need: <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank"
          rel="noopener noreferrer">https://arxiv.org/pdf/1706.03762.pdf</a></li>
      <li>GPT-3: <a href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank"
          rel="noopener noreferrer">https://arxiv.org/pdf/2005.14165.pdf</a></li>
      <li>InstructGPT: <a href="https://arxiv.org/pdf/2203.02155.pdf" target="_blank"
          rel="noopener noreferrer">https://arxiv.org/pdf/2203.02155.pdf</a></li>
      <li>Alpaca: A Strong, Replicable Instruction-Following Model: <a
          href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank"
          rel="noopener noreferrer">https://crfm.stanford.edu/2023/03/13/alpaca.html</a></li>
      <li>Vicuna: <a href="https://lmsys.org/blog/2023-03-30-vicuna/" target="_blank"
          rel="noopener noreferrer">https://lmsys.org/blog/2023-03-30-vicuna/</a></li>
      <li>LoRA: Low-Rank Adaptation of Large Language Models: <a href="https://arxiv.org/pdf/2106.09685.pdf"
          target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2106.09685.pdf</a></li>
      <li>LLaMA: <a href="https://arxiv.org/pdf/2302.13971.pdf" target="_blank"
          rel="noopener noreferrer">https://arxiv.org/pdf/2302.13971.pdf</a></li>
      <li>LLaMA-Adapter: <a href="https://arxiv.org/pdf/2303.16199.pdf" target="_blank"
          rel="noopener noreferrer">https://arxiv.org/pdf/2303.16199.pdf</a></li>
      <li>LLaMA 2: <a href="https://arxiv.org/pdf/2307.09288.pdf" target="_blank"
          rel="noopener noreferrer">https://arxiv.org/pdf/2307.09288.pdf</a></li>
      <li>Dataset for RLHF: <a href="https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view"
          target="_blank"
          rel="noopener noreferrer">https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view</a></li>
      <li>FlashAttention: <a href="https://arxiv.org/pdf/2205.14135.pdf" target="_blank"
          rel="noopener noreferrer">https://arxiv.org/pdf/2205.14135.pdf</a></li>
      <li>Orca: Progressive Learning from Complex Explanation Traces of GPT-4: <a
          href="https://arxiv.org/pdf/2306.02707.pdf" target="_blank"
          rel="noopener noreferrer">https://arxiv.org/pdf/2306.02707.pdf</a></li>
      <li>Quantization: ?</li>
    </ul>

    <p>
      √úber die konkreten Arbeiten und die Liste k√∂nnen wir gerne reden, allerdings m√∂chte ich den Fokus auf Arbeiten
      legen, die eine Grundlage f√ºr
      Large Language Models bzw. Transformer-basierte Sprachmodelle legen, oder darauf aufbauen.
    </p>
    <p>
      Zum Beispiel haben wir dort GPT-3 und InstructGPT, die beide die Grundlagen f√ºr ChatGPT geschafften haben.
      Beide Paper sind auch von OpenAI. Au√üerdem gibt es das √Ñquivalent von Meta: LLaMA und dessen Verbesserungen
      und Weiterentwicklungen (Vicuna, Alpaca, LLaMA-2). Die restlichen Paper sind Techniken, die im
      Zusammenhang mit den genannten Modellen stehen, also Optimierungen oder Trainingsmechaniken etc. sind
      (LoRA, RLHF, FlashAttention, Orca, LLaMA-Adapter). Ich denke, alles, das sich hier in diese Liste gut einf√ºgt,
      kann
      eingebracht werden.
    </p>
    <p>
      Nicht alle davon sind wissenschaftliche Papiere, aber vielleicht eignen sie sich trotzdem.
      Vielleicht aber auch nicht, dann schmei√üen wir die eben wieder raus.
    </p>
    <h4>Voraussetzungen</h4>
    <p>
      Um die Paper zur verstehen, w√§re es wichtig, wenn ihr wisst wie man grunds√§tzlich ein neuronales Netz
      trainiert. Im Idealfall habt ihr schonmal ein CNN oder RNN Modell trainiert oder benutzt, aber da Transformer
      anders aufgebaut sind, ist das jetzt kein Muss, w√ºrde ich sagen.
    </p>

    <h4>Vergangene Treffen</h4>
    <ul id="past-meetings-list"></ul>
    <!-- Filled by JavaScript -->
    <noscript>
      <p>‚ö†Ô∏èDie Informationen zu den vergangenen Treffen konnten nicht geladen werden, da JavaScript deaktiviert ist.</p>
    </noscript>

    <h4>Kontakt</h4>
    <p>Anja: <code>reading-group(at)deep.cooking</code></p>
  </main>

  <footer>
    <p>Made using <a href="https://simplecss.org">Simple.css</a>.</p>
  </footer>
</body>

</html>
