<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ü§ñüí¨ + üìö + ‚òïÔ∏è | ChatGPT@ascii</title>
    <link rel="stylesheet" href="simple.min.css">
</head>
<body>
  <header>

    <h2>ChatGPT Reading Group @ ascii</h2>
      <p>ü§ñüí¨ + üìö + ‚òïÔ∏è</p>
  </header>

  <main>
    <p>Hallo zusammen!</p>
    <p>
        Es gab mal die Idee, gemeinsam die grundlegenden Paper, auf denen ChatGPT aufbaut, zu lesen und dr√ºber
        zu reden. Um das mal zu besprechen, w√ºrde ich vorschlagen, dass wir uns mal im ascii treffen.
    </p>
      <p>
          F√ºr ein erstes Treffen habe ich ein dudle zur Terminfindung rum geschickt. Wer das nicht bekommen hat, kann
          gerne eine E-Mail an reading-group(at)deep.cooking schicken. Das erste Treffen kann auch erstmal rein
          organisatorisch sein. Dann k√∂nnen wir mal sehen, wie viele wir eigentlich sind und ob das Format (siehe unten)
          so passt. Nur der thematische Focus ist fest.
      </p>

      <h4>"Reading Group"</h4>
      <p>
          Die Idee w√§re, dass wir uns regelm√§√üig (zwei w√∂chentlich?) zusammen setzen. Pro Termin gibt es ein Paper o.√§.,
          das alle vorher lesen und im Idealfall auch direkt ein paar offene Punkte, Unklarheiten oder sonstige Fragen
          aufschreiben. Eine Person stellt das Paper dann anhand von ein paar Slides vor (da reichen auch die
          Abbildungen und Tabellen in einer PPT.). Wichtig ist eben, dass wir f√ºr jedes Paper eine/n Verantwortliche/n
          haben, um Fragen zu stellen und ein bisschen den Termin zu strukturieren. Das ist aber auch einfacher als es
          klingt :) Das erste Papier kann ich gerne selbst √ºbernehmen.
      </p>

      <h5>
          Focus
      </h5>
      <p>
          Als Focus f√ºr die Reading Group h√§tte ich, wie oben geschrieben, gerne alles was irgendwie mit ChatGPT und
          den aktuellen Sprachmodellen, die f√ºr Chats und Co. benutzt werden, zu tun hat. Es gibt ja eine Reihe von
          Weiterentwicklungen und Verbesserungen. Die w√ºrde ich mir gerne mal anschauen.
      </p>

      <p>
          Das Ganze soll also keine Lehrveranstaltung sein, auch kein Programmierkurs oder √§hnliches. Einfach nur Paper
          lesen und dann bei einem Kaffee, Tee oder einem Kaltgetr√§nk dar√ºber reden.
      </p>

      <h5>
          Paper
      </h5>
      <p>
        Als Paper w√ºrde ich folgende vorschlagen (nicht zwingend in der Reihenfolge):
      </p>

      <ul>
          <li>Transformers: Attention is All You Need: <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1706.03762.pdf</a></li>
          <li>GPT-3: <a href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2005.14165.pdf</a></li>
          <li>InstructGPT: <a href="https://arxiv.org/pdf/2203.02155.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2203.02155.pdf</a></li>
          <li>Alpaca: A Strong, Replicable Instruction-Following Model: <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" rel="noopener noreferrer">https://crfm.stanford.edu/2023/03/13/alpaca.html</a></li>
          <li>Vicuna: <a href="https://lmsys.org/blog/2023-03-30-vicuna/" target="_blank" rel="noopener noreferrer">https://lmsys.org/blog/2023-03-30-vicuna/</a></li>
          <li>LoRA: Low-Rank Adaptation of Large Language Models: <a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2106.09685.pdf</a></li>
          <li>LLaMA: <a href="https://arxiv.org/pdf/2302.13971.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2302.13971.pdf</a></li>
          <li>LLaMA-Adapter: <a href="https://arxiv.org/pdf/2303.16199.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2303.16199.pdf</a></li>
          <li>LLaMA 2: <a href="https://arxiv.org/pdf/2307.09288.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2307.09288.pdf</a></li>
          <li>Dataset for RLHF: <a href="https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view" target="_blank" rel="noopener noreferrer">https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view</a></li>
          <li>FlashAttention: <a href="https://arxiv.org/pdf/2205.14135.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2205.14135.pdf</a></li>
          <li>Orca: Progressive Learning from Complex Explanation Traces of GPT-4: <a href="https://arxiv.org/pdf/2306.02707.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2306.02707.pdf</a></li>
      </ul>

      <p>
        √úber die konkreten Arbeiten und die Liste k√∂nnen wir gerne reden, allerdings m√∂chte ich den Fokus auf Arbeiten legen, die eine Grundlage f√ºr
          Large Language Models bzw. Transformer-basierte Sprachmodelle legen, oder darauf aufbauen.
      </p>
      <p>
          Zum Beispiel haben wir dort GPT-3 und InstructGPT, die beide die Grundlagen f√ºr ChatGPT geschafften haben.
          Beide Paper sind auch von OpenAI. Au√üerdem gibt es das √Ñquivalent von Meta: LLaMA und dessen Verbesserungen
          und Weiterentwicklungen (Vicuna, Alpaca, LLaMA-2). Die restlichen Paper sind Techniken, die im
          Zusammenhang mit den genannten Modellen stehen, also Optimierungen oder Trainingsmechaniken etc. sind
          (LoRA, RLHF, FlashAttention, Orca, LLaMA-Adapter). Ich denke, alles, das sich hier in diese Liste gut einf√ºgt, kann
          eingebracht werden.
      </p>
      <p>
          Nicht alle davon sind wissenschaftliche Papiere, aber vielleicht eignen sie sich trotzdem.
          Vielleicht aber auch nicht, dann schmei√üen wir die eben wieder raus.
      </p>

    <h4>Kontakt</h4>
    <p>Anja: reading-group(at)deep.cooking</p>
  </main>

  <footer>
    <p>Made using <a href="https://simplecss.org">Simple.css</a>.</p>
  </footer>
</body>
</html>